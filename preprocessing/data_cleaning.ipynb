{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1929aef",
   "metadata": {},
   "source": [
    "# Tweet Sentiment Analysis with BERT\n",
    "\n",
    "Performing sentiment analysis using BERT (fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e9cc52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9af70b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Loong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Loong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Loong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources if not already done\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "29747035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>@tiniebeany climate change is an interesting h...</td>\n",
       "      <td>792927353886371840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @NatGeoChannel: Watch #BeforeTheFlood right...</td>\n",
       "      <td>793124211518832641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Fabulous! Leonardo #DiCaprio's film on #climat...</td>\n",
       "      <td>793124402388832256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @Mick_Fanning: Just watched this amazing do...</td>\n",
       "      <td>793124635873275904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @cnalive: Pranita Biswasi, a Lutheran from ...</td>\n",
       "      <td>793125156185137153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  \\\n",
       "0         -1  @tiniebeany climate change is an interesting h...   \n",
       "1          1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n",
       "2          1  Fabulous! Leonardo #DiCaprio's film on #climat...   \n",
       "3          1  RT @Mick_Fanning: Just watched this amazing do...   \n",
       "4          2  RT @cnalive: Pranita Biswasi, a Lutheran from ...   \n",
       "\n",
       "              tweetid  \n",
       "0  792927353886371840  \n",
       "1  793124211518832641  \n",
       "2  793124402388832256  \n",
       "3  793124635873275904  \n",
       "4  793125156185137153  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data\n",
    "data = pd.read_csv('../data/data.csv')\n",
    "\n",
    "# preview the ddata\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9138bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns and duplicates\n",
    "data = data.drop_duplicates(subset=['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "05b79cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tools\n",
    "tokenizer = TweetTokenizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f5687",
   "metadata": {},
   "source": [
    "# Selective cleaning\n",
    "### Keeping hashtag words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21a26b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\brt\\b', '<rt>', text)  # Replace 'RT' retweet tag with <rt>\n",
    "    text = re.sub(r'@\\w+', '<mention>', text)  # Replace @mentions with <mention>\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '<url>', text, flags=re.MULTILINE)  # Replace URLs with <url>\n",
    "    text = re.sub(r'#', '', text)  # Remove only the '#' symbol, keep the hashtag word\n",
    "    text = re.sub(r'[^a-zA-Z<>\\s]', '', text)  # Remove special characters and numbers, keep < and > for tags\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "\n",
    "    # Remove stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    text = ' '.join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f6ff550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             message  \\\n",
      "0  @tiniebeany climate change is an interesting h...   \n",
      "1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n",
      "2  Fabulous! Leonardo #DiCaprio's film on #climat...   \n",
      "3  RT @Mick_Fanning: Just watched this amazing do...   \n",
      "4  RT @cnalive: Pranita Biswasi, a Lutheran from ...   \n",
      "\n",
      "                                       clean_message  \n",
      "0  <mention> climate change interesting hustle gl...  \n",
      "1  <rt> <mention> watch beforetheflood right <men...  \n",
      "2  fabulous leonardo dicaprios film climate chang...  \n",
      "3  <rt> <mention> watched amazing documentary leo...  \n",
      "4  <rt> <mention> pranita biswasi lutheran odisha...  \n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning and store in a new DataFrame\n",
    "data_selective_clean = data.copy()\n",
    "data_selective_clean['clean_message'] = data_selective_clean['message'].apply(clean_text)\n",
    "\n",
    "# Display the cleaned data\n",
    "print(data_selective_clean[['message', 'clean_message']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "data_selective_clean.to_csv('../data/data_selective_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90dcdfa",
   "metadata": {},
   "source": [
    "### Clean all symbols, mentions, hashtags, special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9ca092fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\brt\\b', '', text)  # Remove 'RT' retweet tag\n",
    "    text = re.sub(r'@\\w+', '', text)    # Remove @mentions entirely\n",
    "    text = re.sub(r'#\\w+', '', text)    # Remove hashtags and hashtag words entirely\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
    "\n",
    "    # Remove stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "023aee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             message  \\\n",
      "0  @tiniebeany climate change is an interesting h...   \n",
      "1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n",
      "2  Fabulous! Leonardo #DiCaprio's film on #climat...   \n",
      "3  RT @Mick_Fanning: Just watched this amazing do...   \n",
      "4  RT @cnalive: Pranita Biswasi, a Lutheran from ...   \n",
      "\n",
      "                                       clean_message  \n",
      "0  climate change interesting hustle global warmi...  \n",
      "1  watch right travel world tackle climate change...  \n",
      "2  fabulous leonardo film change brilliant watch via  \n",
      "3  watched amazing documentary leonardodicaprio c...  \n",
      "4  pranita biswasi lutheran odisha give testimony...  \n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning and store in a new DataFrame\n",
    "data_clean_all = data.copy()\n",
    "data_clean_all['clean_message'] = data_selective_clean['message'].apply(clean_text)\n",
    "\n",
    "# Display the cleaned data\n",
    "print(data_clean_all[['message', 'clean_message']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3637c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "data_clean_all.to_csv('../data/data_clean_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb475a2",
   "metadata": {},
   "source": [
    "# Please save your data in the `data` folder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
